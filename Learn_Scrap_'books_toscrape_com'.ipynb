{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTEWAta59yNCdprzYcF6Bi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikolaswidad/Learning-Data-Mining/blob/main/Learn_Scrap_'books_toscrape_com'.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUNlnd2LXmHf"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "TpkuAtQHYP96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dl7jbn5OX8LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "Hh-_U1Ctn1Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = []\n",
        "for i in range(1, 2):\n",
        "  url = f\"https://books.toscrape.com/catalogue/page-{i}.html\"\n",
        "  response = requests.get(url)\n",
        "  response = response.content\n",
        "  soup = BeautifulSoup(response, 'html.parser')\n",
        "  ol = soup.find('ol')\n",
        "  articles = ol.find_all('article', class_='product_pod')\n",
        "\n",
        "  for article in articles:\n",
        "    image = article.find('img')\n",
        "    title = image.attrs['alt']\n",
        "    star = article.find('p')\n",
        "    star = star['class'][1]\n",
        "    price = article.find('p', class_='price_color').text\n",
        "    price = float(price[1:])\n",
        "    # print(price)\n",
        "    books.append([title, price, star])\n",
        "# print(articles)"
      ],
      "metadata": {
        "id": "SzXN8WZcZRdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(books, columns=['Ttile', 'Price', 'Star Rating'])"
      ],
      "metadata": {
        "id": "hJ0X4GLSZUWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('books.csv')"
      ],
      "metadata": {
        "id": "W6K7yh0ncE1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL yang akan Anda scrape\n",
        "url = \"https://www.vidio.com/tags/sports-stream/lives\"\n",
        "\n",
        "# Lakukan permintaan HTTP ke URL\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Temukan semua elemen dengan class \"home-content\"\n",
        "livestreams = soup.find_all('div', class_='home-content')\n",
        "\n",
        "sport_streams = []\n",
        "# Loop melalui setiap elemen dan ekstrak informasi yang Anda inginkan\n",
        "for livestream in livestreams:\n",
        "    # Ekstrak tautan dengan mencari elemen 'a' yang memiliki atribut 'href'\n",
        "    link = livestream.find('a')['href']\n",
        "    link = \"https://www.vidio.com\" + link\n",
        "\n",
        "    # Ekstrak judul dengan mencari elemen dengan class 'home-content__title'\n",
        "    title_element = livestream.find('div', class_='home-content__title')\n",
        "    title = title_element.text if title_element else \"\"\n",
        "\n",
        "    # Memisahkan teks berdasarkan tanda penghubung (-)\n",
        "    parts_title = title.split(\" - \")\n",
        "\n",
        "    if len(parts_title) == 2:\n",
        "        title = parts_title[0]\n",
        "        event = parts_title[1]\n",
        "    else:\n",
        "        title = title  # Biarkan judul seperti apa adanya\n",
        "        event = \"\"  # Set event ke string kosong\n",
        "\n",
        "    # Ekstrak subtitle dengan mencari elemen dengan class 'home-content__subtitle'\n",
        "    subtitle_element = livestream.find('div', class_='home-content__subtitle')\n",
        "    subtitle = subtitle_element.text if subtitle_element else \"\"\n",
        "\n",
        "    # Memisahkan subtitle dengan \" · \" sebagai pemisah\n",
        "    parts_subtitle = subtitle.split(\" · \")\n",
        "\n",
        "    # Menginisialisasi variabel streamer, date, dan time\n",
        "    streamer = \"\"\n",
        "    date = \"\"\n",
        "    time = \"\"\n",
        "\n",
        "    # Memeriksa apakah ada lebih dari satu bagian setelah pemisahan\n",
        "    if len(parts_subtitle) > 1:\n",
        "        # Memisahkan tanggal dan waktu dengan \" - \"\n",
        "        date_time_parts = parts_subtitle[1].split(\" - \")\n",
        "\n",
        "        # Memeriksa apakah ada dua bagian setelah pemisahan kedua\n",
        "        if len(date_time_parts) == 2:\n",
        "            streamer = parts_subtitle[0]\n",
        "            date = date_time_parts[0]\n",
        "            time = date_time_parts[1]\n",
        "        else:\n",
        "            # Jika hanya satu bagian setelah pemisahan kedua, itu akan menjadi tanggal\n",
        "            streamer = parts_subtitle[0]\n",
        "            date = date_time_parts[0]\n",
        "    else:\n",
        "        # Jika hanya satu bagian setelah pemisahan pertama, itu akan menjadi streamer\n",
        "        streamer = parts_subtitle[0]\n",
        "\n",
        "\n",
        "    # Gunakan regex untuk mengekstrak schedule_id dari tautan\n",
        "    schedule_id_match = re.search(r'schedule_id=(\\d+)', link)\n",
        "    if schedule_id_match:\n",
        "        schedule_id = schedule_id_match.group(1)\n",
        "    #     print(\"schedule_id:\", schedule_id)\n",
        "\n",
        "\n",
        "    sport_streams.append([schedule_id, title, event, date, time, streamer, link])\n",
        "\n",
        "dfSports = pd.DataFrame(sport_streams, columns=['Schedule ID', 'Title','Event', 'Date', 'Time', 'Streamer', 'Link'])\n",
        "dfSports.to_csv('sports streams.csv')"
      ],
      "metadata": {
        "id": "Y4HzEtkOiQh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6sSY8DoftyLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfSports = pd.DataFrame(sport_streams, columns=['Title','Event','Schedule ID','Streamer', 'Date', 'Time', 'Link'])"
      ],
      "metadata": {
        "id": "otsJ-IYvpK_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfSports.to_csv('sports streams.csv')"
      ],
      "metadata": {
        "id": "GscNY0rIp8ib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}